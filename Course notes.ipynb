{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Factory Fundamentals\n",
    "\n",
    "It's a web tool to manage complete complex ETL projects.\n",
    "It also includes integration services.\n",
    "\n",
    "We can use it to process a large volume of data.\n",
    "\n",
    "We can use it to retrieve data from local DB's.\n",
    "\n",
    "Data Factory will participate in all the ETL process bringing the data from a raw state to a curated state. For this state changes Data Factory uses Databricks.\n",
    "\n",
    "Finally we have to store the enriched data in a centralized data warehouse, it is also managed by Data Factory.\n",
    "\n",
    "## Data storage\n",
    "\n",
    "It's a centralized data storage that can store structured, semi-structured, non-structured and realtime data.\n",
    "\n",
    "Azure Data factory is used as an Orchestrator for the complete process of the data\n",
    "\n",
    "## Data lake\n",
    "\n",
    "Is a data storage for raw data.\n",
    "\n",
    "## Databricks \n",
    "\n",
    "Service to compute, it will help us to manage the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineers\n",
    "\n",
    "Responsible to organize the data so that others can easily access it.\n",
    "\n",
    "They have to work with different people/roles.\n",
    "\n",
    "Main responsibilities:\n",
    "* Get requirements of the data\n",
    "* Maintain meta data\n",
    "* Ensure security for the data\n",
    "* Data storage\n",
    "* Data processing\n",
    "* ETL tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Data Factory Pipelines\n",
    "\n",
    "It's a flow of activities that perform an operation\n",
    "Commonly they have four steps:\n",
    "\n",
    "1. Collect and store the raw data in a centralized source (Azure Data Lake)\n",
    "2. Transform/Processing of the data\n",
    "3. Publish the enriched information\n",
    "4. Monitoring the pipeline\n",
    "\n",
    "Inside the pipelines we can find different activities, linked services, data Flows and Datasets\n",
    "\n",
    "### Activities\n",
    "\n",
    "Individual steps within a pipeline in which every activity completes a single task\n",
    "\n",
    "There can be executed as sequential or parallel activities within the same pipeline\n",
    "\n",
    "They allow us to move or transform data and also to perform external tasks with services. \n",
    "\n",
    "### Data Flows\n",
    "\n",
    "They are a type of activity.\n",
    "\n",
    "With them we can create data transformations easily.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "They are views that represent groups of data.\n",
    "\n",
    "### Linked Services\n",
    "\n",
    "Connection from Data Factory to external resources.\n",
    "\n",
    "# Triggers\n",
    "\n",
    "With them we define the conditions to start a new pipeline\n",
    "\n",
    "# Templates\n",
    "\n",
    "They are templates for standard pipelines and are defined by Microsoft."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
